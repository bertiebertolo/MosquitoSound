{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVN8C71jv9FQ"
      },
      "source": [
        "## Importing classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import librosa\n",
        "from librosa.util import normalize\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "C:\\Users\\alber\\OneDrive\\Desktop\\Sound analysis 2\\Wav_data_all\\Culex\\forward\\2025_04_03_NI_Cxpp_male_mosq1_-30V_No injection_5 min_270-27_LSwAv_108_forward_1.wav\n",
            "C:\\Users\\alber\\OneDrive\\Desktop\\Sound analysis 2\\Wav_data_all\\Culex\\forward\\2025_04_03_NI_Cxpp_male_mosq1_-30V_No injection_5 min_270-27_LSwAv_108_forward_2.wav\n",
            "C:\\Users\\alber\\OneDrive\\Desktop\\Sound analysis 2\\Wav_data_all\\Culex\\forward\\2025_04_03_NI_Cxpp_male_mosq1_-30V_No injection_5 min_270-27_LSwAv_10_forward_1.wav\n",
            "C:\\Users\\alber\\OneDrive\\Desktop\\Sound analysis 2\\Wav_data_all\\Culex\\forward\\2025_04_03_NI_Cxpp_male_mosq1_-30V_No injection_5 min_270-27_LSwAv_10_forward_2.wav\n",
            "C:\\Users\\alber\\OneDrive\\Desktop\\Sound analysis 2\\Wav_data_all\\Culex\\forward\\2025_04_03_NI_Cxpp_male_mosq1_-30V_No injection_5 min_270-27_LSwAv_15_forward_1.wav\n",
            "C:\\Users\\alber\\OneDrive\\Desktop\\Sound analysis 2\\Wav_data_all\\Culex\\forward\\2025_04_03_NI_Cxpp_male_mosq1_-30V_No injection_5 min_270-27_LSwAv_15_forward_2.wav\n",
            "C:\\Users\\alber\\OneDrive\\Desktop\\Sound analysis 2\\Wav_data_all\\Culex\\forward\\2025_04_03_NI_Cxpp_male_mosq1_-30V_No injection_5 min_270-27_LSwAv_165_forward_1.wav\n",
            "C:\\Users\\alber\\OneDrive\\Desktop\\Sound analysis 2\\Wav_data_all\\Culex\\forward\\2025_04_03_NI_Cxpp_male_mosq1_-30V_No injection_5 min_270-27_LSwAv_165_forward_2.wav\n",
            "C:\\Users\\alber\\OneDrive\\Desktop\\Sound analysis 2\\Wav_data_all\\Culex\\forward\\2025_04_03_NI_Cxpp_male_mosq1_-30V_No injection_5 min_270-27_LSwAv_170_forward_1.wav\n",
            "C:\\Users\\alber\\OneDrive\\Desktop\\Sound analysis 2\\Wav_data_all\\Culex\\forward\\2025_04_03_NI_Cxpp_male_mosq1_-30V_No injection_5 min_270-27_LSwAv_170_forward_2.wav\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Path to the directory containing .wav files\n",
        "directory = r\"C:\\Users\\alber\\OneDrive\\Desktop\\Sound analysis 2\\Wav_data_all\\Culex\\forward\"\n",
        "\n",
        "# List all .wav files in the directory\n",
        "wav_files = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.wav')]\n",
        "\n",
        "# Split the files into train and test sets\n",
        "train_files = wav_files[9:]  # Skipping the first 9 files for training\n",
        "test_files = wav_files[:10]  # Taking the first 10 files for testing\n",
        "\n",
        "# Save the paths to text files\n",
        "with open('train_files.txt', 'w') as f:\n",
        "    for file in train_files:\n",
        "        f.write(f\"{file}\\n\")\n",
        "\n",
        "with open('test_files.txt', 'w') as f:\n",
        "    for file in test_files:\n",
        "        f.write(f\"{file}\\n\")\n",
        "\n",
        "# Print the test files\n",
        "with open('test_files.txt', 'r') as f:\n",
        "    print(f.read())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-06T08:14:22.710460Z",
          "iopub.status.busy": "2023-07-06T08:14:22.710040Z",
          "iopub.status.idle": "2023-07-06T08:14:22.764007Z",
          "shell.execute_reply": "2023-07-06T08:14:22.763089Z",
          "shell.execute_reply.started": "2023-07-06T08:14:22.710426Z"
        },
        "id": "chWzunwzIuEL",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import librosa\n",
        "import math\n",
        "import torch\n",
        "import torch.utils.data\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils import weight_norm\n",
        "from librosa.core import load\n",
        "#from librosa.core import loadmel_basis\n",
        "from librosa.util import normalize\n",
        "\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import random\n",
        "##\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "from librosa.filters import mel as librosa_mel_fn\n",
        "from torch.nn.utils import weight_norm\n",
        "import numpy as np\n",
        "import scipy.io.wavfile\n",
        "#########"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "18em7XB6IuEL",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.11.0\n"
          ]
        }
      ],
      "source": [
        "import librosa\n",
        "print(librosa.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-06T08:14:27.860060Z",
          "iopub.status.busy": "2023-07-06T08:14:27.859698Z",
          "iopub.status.idle": "2023-07-06T08:14:27.896349Z",
          "shell.execute_reply": "2023-07-06T08:14:27.895340Z",
          "shell.execute_reply.started": "2023-07-06T08:14:27.860031Z"
        },
        "id": "YtIMm_qmIuEM",
        "outputId": "df00aa65-28d6-490d-99a7-6867e7753fb5",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Type of audio_files: <class 'list'>\n",
            "Number of audio files: 163\n",
            "Processing file: C:\\Users\\alber\\OneDrive\\Desktop\\Sound analysis 2\\Wav_data_all\\Culex\\forward\\2025_04_04_NI_Cxpp_male_mosq2_-30V_No injection_5 min_281-28_LSwAv_0_forward_2.wav\n",
            "Saved audio to: C:\\Users\\alber\\OneDrive\\Desktop\\Sound analysis 2\\Wav_data_all\\Culex\\forward\\2025_04_04_NI_Cxpp_male_mosq2_-30V_No injection_5 min_281-28_LSwAv_0_forward_2.wav\n",
            "Processing file: C:\\Users\\alber\\OneDrive\\Desktop\\Sound analysis 2\\Wav_data_all\\Culex\\forward\\2025_04_04_NI_Cxpp_male_mosq2_-30V_No injection_5 min_281-28_LSwAv_62_forward_1.wav\n",
            "Saved audio to: C:\\Users\\alber\\OneDrive\\Desktop\\Sound analysis 2\\Wav_data_all\\Culex\\forward\\2025_04_04_NI_Cxpp_male_mosq2_-30V_No injection_5 min_281-28_LSwAv_62_forward_1.wav\n",
            "Processing file: C:\\Users\\alber\\OneDrive\\Desktop\\Sound analysis 2\\Wav_data_all\\Culex\\forward\\2025_04_04_NI_Cxpp_male_mosq2_-30V_No injection_5 min_281-28_LSwAv_183_forward_2.wav\n",
            "Saved audio to: C:\\Users\\alber\\OneDrive\\Desktop\\Sound analysis 2\\Wav_data_all\\Culex\\forward\\2025_04_04_NI_Cxpp_male_mosq2_-30V_No injection_5 min_281-28_LSwAv_183_forward_2.wav\n",
            "Processing file: C:\\Users\\alber\\OneDrive\\Desktop\\Sound analysis 2\\Wav_data_all\\Culex\\forward\\2025_04_04_NI_Cxpp_male_mosq3_-30V_No injection_5 min_279-28_LSwAv_311_forward_2.wav\n",
            "Saved audio to: C:\\Users\\alber\\OneDrive\\Desktop\\Sound analysis 2\\Wav_data_all\\Culex\\forward\\2025_04_04_NI_Cxpp_male_mosq3_-30V_No injection_5 min_279-28_LSwAv_311_forward_2.wav\n",
            "Processing file: C:\\Users\\alber\\OneDrive\\Desktop\\Sound analysis 2\\Wav_data_all\\Culex\\forward\\2025_04_04_NI_Cxpp_male_mosq1_-30V_No injection_5 min_283-28_LSwAv_112_forward_1.wav\n",
            "Saved audio to: C:\\Users\\alber\\OneDrive\\Desktop\\Sound analysis 2\\Wav_data_all\\Culex\\forward\\2025_04_04_NI_Cxpp_male_mosq1_-30V_No injection_5 min_283-28_LSwAv_112_forward_1.wav\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import scipy.io.wavfile\n",
        "from pathlib import Path\n",
        "\n",
        "# Helper function to convert a text file of filenames into a list of filenames\n",
        "def files_to_list(filename):\n",
        "    \"\"\"\n",
        "    Takes a text file of filenames and makes a list of filenames\n",
        "    \"\"\"\n",
        "    with open(filename, encoding=\"utf-8\") as f:\n",
        "        files = f.readlines()\n",
        "    files = [f.rstrip() for f in files]  # Remove trailing spaces and newlines\n",
        "    return files\n",
        "\n",
        "# Path to the text file containing the list of audio files (assuming it's in the same working directory)\n",
        "filename = Path(\"train_files.txt\")  # Modify this if the text file is named differently\n",
        "\n",
        "# Read the file paths from the text file and shuffle them\n",
        "audio_files = files_to_list(filename)\n",
        "print(f\"Type of audio_files: {type(audio_files)}\")\n",
        "print(f\"Number of audio files: {len(audio_files)}\")\n",
        "\n",
        "# Set a seed for reproducibility and shuffle the list of audio file paths\n",
        "random.seed(1234)\n",
        "random.shuffle(audio_files)\n",
        "\n",
        "# Helper function to save a sample (audio) as a .wav file\n",
        "def save_sample(file_path, sampling_rate, audio):\n",
        "    \"\"\"\n",
        "    Helper function to save sample audio to a file.\n",
        "\n",
        "    Args:\n",
        "        file_path (str or pathlib.Path): path where the file will be saved.\n",
        "        sampling_rate (int): sampling rate of the audio (usually 22050 or 44100).\n",
        "        audio (torch.FloatTensor): normalized audio in the range of [-1, 1].\n",
        "    \"\"\"\n",
        "    audio = (audio.numpy() * 32768).astype(\"int16\")  # Convert to 16-bit integer format\n",
        "    scipy.io.wavfile.write(file_path, sampling_rate, audio)  # Save the audio as a .wav file\n",
        "\n",
        "# Example: Processing and saving the first few audio files in the shuffled list\n",
        "for i, file in enumerate(audio_files[:5]):  # Let's process the first 5 files for this example\n",
        "    print(f\"Processing file: {file}\")\n",
        "\n",
        "    # Load the audio file (this is an example, ensure that `load_audio` is implemented)\n",
        "    # Example: Let's assume we load the audio into a tensor.\n",
        "    # You can use `librosa` or any other library to load the audio data into a tensor.\n",
        "    audio_data, sampling_rate = torch.randn(1, 1, 88200), 22050  # Dummy audio data (replace this with actual loading)\n",
        "    \n",
        "    # Save the audio sample as a .wav file\n",
        "    save_sample(file, sampling_rate, audio_data)\n",
        "    print(f\"Saved audio to: {file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-06T08:14:33.433626Z",
          "iopub.status.busy": "2023-07-06T08:14:33.433251Z",
          "iopub.status.idle": "2023-07-06T08:14:33.446204Z",
          "shell.execute_reply": "2023-07-06T08:14:33.445109Z",
          "shell.execute_reply.started": "2023-07-06T08:14:33.433594Z"
        },
        "id": "LxKKB_yFIuEM",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 100000])\n"
          ]
        }
      ],
      "source": [
        "# Assuming the 'audio_files' list has already been defined in the first cell\n",
        "class AudioDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    This is the main class that calculates the spectrogram and returns the\n",
        "    spectrogram, audio pair.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, audio_files, segment_length, sampling_rate, augment=True):\n",
        "        self.sampling_rate = sampling_rate\n",
        "        self.segment_length = segment_length\n",
        "        self.audio_files = audio_files  # Directly using the audio_files passed from the first cell\n",
        "        random.seed(1234)\n",
        "        random.shuffle(self.audio_files)  # Shuffle the files\n",
        "        self.augment = augment\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Read audio\n",
        "        filename = self.audio_files[index]\n",
        "        audio, sampling_rate = self.load_wav_to_torch(filename)\n",
        "        \n",
        "        # Take segment\n",
        "        if audio.size(0) >= self.segment_length:\n",
        "            max_audio_start = audio.size(0) - self.segment_length\n",
        "            audio_start = random.randint(0, max_audio_start)\n",
        "            audio = audio[audio_start: audio_start + self.segment_length]\n",
        "        else:\n",
        "            audio = F.pad(audio, (0, self.segment_length - audio.size(0)), \"constant\").data\n",
        "\n",
        "        return audio.unsqueeze(0)  # Unsqueeze to add the batch dimension\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audio_files)\n",
        "\n",
        "    def load_wav_to_torch(self, full_path):\n",
        "        \"\"\"\n",
        "        Loads wavdata into torch array\n",
        "        \"\"\"\n",
        "        data, sampling_rate = librosa.load(full_path, sr=self.sampling_rate)\n",
        "        data = 0.95 * normalize(data)\n",
        "\n",
        "        if self.augment:\n",
        "            amplitude = np.random.uniform(low=0.3, high=1.0)\n",
        "            data = data * amplitude\n",
        "\n",
        "        return torch.from_numpy(data).float(), sampling_rate\n",
        "\n",
        "# Example usage (make sure you have the 'audio_files' list from the first cell):\n",
        "# Instantiate the dataset\n",
        "audio_dataset = AudioDataset(audio_files=audio_files, segment_length=100000, sampling_rate=100000)\n",
        "\n",
        "# Example: Access a single item from the dataset\n",
        "sample_audio = audio_dataset[0]\n",
        "print(sample_audio.shape)  # The shape of the output tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output shape: torch.Size([1, 100000])\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import librosa\n",
        "from librosa.util import normalize\n",
        "from pathlib import Path\n",
        "\n",
        "# 1) Prepare the list of WAV‐file paths\n",
        "wav_dir = Path(r\"C:\\Users\\alber\\OneDrive\\Desktop\\Sound analysis 2\\Wav_data_all\\Culex\\forward\")\n",
        "audio_files = [str(p) for p in wav_dir.glob(\"*.wav\")]\n",
        "\n",
        "# 2) Set sample rate and 1 sec length\n",
        "sampling_rate  = 100_000\n",
        "segment_length = sampling_rate  # exactly 1 second\n",
        "\n",
        "# 3) Define the dataset using your AudioDataset\n",
        "dataset = AudioDataset(\n",
        "    audio_files=audio_files,\n",
        "    segment_length=segment_length,  # now 100000 samples\n",
        "    sampling_rate=sampling_rate,    # load/resample at 100 kHz\n",
        "    augment=True\n",
        ")\n",
        "\n",
        "# 4) Fetch one sample and print its shape\n",
        "sample_audio = dataset[0]   # → torch.Size([1, 100000])\n",
        "print(\"Output shape:\", sample_audio.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "n_mel_channels = 40\n",
        "def librosa_mel_fn(sampling_rate, n_fft, n_mel_channels, mel_fmin, mel_fmax):\n",
        "    mel_basis = librosa.filters.mel(\n",
        "        sr=sampling_rate,\n",
        "        n_fft=n_fft,\n",
        "        n_mels=n_mel_channels,\n",
        "        fmin=mel_fmin,\n",
        "        fmax=mel_fmax\n",
        "    )\n",
        "    return mel_basis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLdbB3k9IuEO"
      },
      "source": [
        "# MRH: Modified Audio2Mel:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-06T08:14:40.341315Z",
          "iopub.status.busy": "2023-07-06T08:14:40.340944Z",
          "iopub.status.idle": "2023-07-06T08:14:40.352846Z",
          "shell.execute_reply": "2023-07-06T08:14:40.351588Z",
          "shell.execute_reply.started": "2023-07-06T08:14:40.341282Z"
        },
        "id": "r3g13ypXIuEO",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#######################################################################################################3\n",
        "\n",
        "class mrh_Audio2Mel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_fft=1024,\n",
        "        hop_length=256,\n",
        "        win_length=1024,\n",
        "        sampling_rate=100000,\n",
        "        n_mel_channels=30,\n",
        "        mel_fmin=0.0,\n",
        "        mel_fmax= 2000,  # Default is None, set inside the constructor\n",
        "    ):\n",
        "        super().__init__()\n",
        "        ##############################################\n",
        "        # FFT Parameters                              #\n",
        "        ##############################################\n",
        "        window = torch.hann_window(win_length).float()\n",
        "        mel_basis = librosa_mel_fn(\n",
        "            sampling_rate, n_fft, n_mel_channels, mel_fmin, mel_fmax\n",
        "        )\n",
        "        mel_basis = torch.from_numpy(mel_basis).float()\n",
        "        #print('mel_basis.size()= ',mel_basis.size())\n",
        "        self.register_buffer(\"mel_basis\", mel_basis)\n",
        "        self.register_buffer(\"window\", window)\n",
        "        self.n_fft = n_fft\n",
        "        self.hop_length = hop_length\n",
        "        self.win_length = win_length\n",
        "        self.sampling_rate = sampling_rate\n",
        "        self.n_mel_channels = n_mel_channels\n",
        "\n",
        "    def forward(self, audio):\n",
        "        #print('mrh1 = ',audio.size())\n",
        "        p = (self.n_fft - self.hop_length) // 2\n",
        "        audio = F.pad(audio, (p, p), \"reflect\").squeeze(1)\n",
        "        fft = torch.stft(\n",
        "            audio,\n",
        "            n_fft=self.n_fft,\n",
        "            hop_length=self.hop_length,\n",
        "            win_length=self.win_length,\n",
        "            window=self.window,\n",
        "            return_complex=True,\n",
        "            center=False,\n",
        "        )\n",
        "        #print('mrh1 = ',fft.size())\n",
        "        #print('\\nmrh2 = ',fft)\n",
        "        #print('\\nmrh22= ',fft.size())\n",
        "        #MRH:\n",
        "        #real_part, imag_part = fft.unbind(-1)\n",
        "        #magnitude = torch.sqrt(real_part ** 2 + imag_part ** 2)\n",
        "        x=fft\n",
        "        x_real=x.real\n",
        "        x_img=x.imag\n",
        "        magnitude = torch.sqrt(x_real** 2 + x_img** 2)\n",
        "        #print('magnitude.size() = ',magnitude.size())\n",
        "\n",
        "        mel_output = torch.matmul(self.mel_basis, magnitude)\n",
        "        #print('mel_output.size() = ',mel_output.size())\n",
        "        log_mel_spec = torch.log10(torch.clamp(mel_output, min=1e-5))\n",
        "        #print('log_mel_spec.size() = ',log_mel_spec.size())\n",
        "        return log_mel_spec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "87D8utPfIuEO",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x = tensor([[[0.9679+0.2413j, 0.3107+0.2208j, 0.1124+0.5299j],\n",
            "         [0.6305+0.0698j, 0.0243+0.9528j, 0.5247+0.8242j],\n",
            "         [0.7688+0.2525j, 0.2749+0.6809j, 0.1601+0.4715j],\n",
            "         [0.9079+0.0250j, 0.2360+0.8872j, 0.9586+0.1765j],\n",
            "         [0.7937+0.2371j, 0.1190+0.0659j, 0.0145+0.8026j]]])\n",
            "shape = torch.Size([1, 5, 3])\n",
            "\n",
            "Real part: tensor([[[0.9679, 0.3107, 0.1124],\n",
            "         [0.6305, 0.0243, 0.5247],\n",
            "         [0.7688, 0.2749, 0.1601],\n",
            "         [0.9079, 0.2360, 0.9586],\n",
            "         [0.7937, 0.1190, 0.0145]]])\n",
            "\n",
            "Imag part: tensor([[[0.2413, 0.2208, 0.5299],\n",
            "         [0.0698, 0.9528, 0.8242],\n",
            "         [0.2525, 0.6809, 0.4715],\n",
            "         [0.0250, 0.8872, 0.1765],\n",
            "         [0.2371, 0.0659, 0.8026]]])\n",
            "\n",
            "Magnitude1 (sqrt(real^2 + imag^2)): tensor([[[0.9976, 0.3812, 0.5417],\n",
            "         [0.6343, 0.9532, 0.9771],\n",
            "         [0.8092, 0.7343, 0.4980],\n",
            "         [0.9082, 0.9181, 0.9747],\n",
            "         [0.8284, 0.1360, 0.8028]]])\n",
            "\n",
            "Magnitude2 (torch.abs(x)): tensor([[[0.9976, 0.3812, 0.5417],\n",
            "         [0.6343, 0.9532, 0.9771],\n",
            "         [0.8092, 0.7343, 0.4980],\n",
            "         [0.9082, 0.9181, 0.9747],\n",
            "         [0.8284, 0.1360, 0.8028]]])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Generate a random complex tensor x\n",
        "x = torch.rand(1, 5, 3, dtype=torch.cfloat)\n",
        "print('x =', x)\n",
        "print('shape =', x.shape)\n",
        "\n",
        "# Extract real and imaginary parts\n",
        "x_real = x.real\n",
        "x_imag = x.imag\n",
        "print('\\nReal part:', x_real)\n",
        "print('\\nImag part:', x_imag)\n",
        "\n",
        "# Correct magnitude calculation via real & imag\n",
        "magnitude1 = torch.sqrt(x_real**2 + x_imag**2)\n",
        "print('\\nMagnitude1 (sqrt(real^2 + imag^2)):', magnitude1)\n",
        "\n",
        "# Even simpler: use torch.abs on a complex tensor\n",
        "magnitude2 = torch.abs(x)\n",
        "print('\\nMagnitude2 (torch.abs(x)):', magnitude2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "BJ_YPf6sIuEO",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\alber\\AppData\\Local\\Temp\\ipykernel_86652\\4284454381.py:5: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.\n",
            "  mel_basis = librosa.filters.mel(\n"
          ]
        }
      ],
      "source": [
        "n_mel_channels=80\n",
        "fft = mrh_Audio2Mel(n_mel_channels=n_mel_channels).cuda()\n",
        "x_t=torch.rand(1,1,88200)\n",
        "x_t = x_t.cuda()\n",
        "s_t = fft(x_t)#.detach()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "L7l6OkbwIuEP",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class Audio2MFCC(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_fft=1024,\n",
        "        hop_length=256,\n",
        "        win_length=1024,\n",
        "        sampling_rate=100000,\n",
        "        n_mel_channels=30,\n",
        "        mel_fmin=0.0,\n",
        "        mel_fmax= 2000,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        ##############################################\n",
        "        # FFT Parameters                              #\n",
        "        ##############################################\n",
        "\n",
        "        self.n_fft = n_fft\n",
        "        self.hop_length = hop_length\n",
        "        self.win_length = win_length\n",
        "        self.sampling_rate = sampling_rate\n",
        "        self.n_mel_channels = n_mel_channels\n",
        "\n",
        "    def forward(self, x):\n",
        "        y=x[0,:,:]\n",
        "        #    print('x= ',x.size())\n",
        "        #    print('y= ',y.size())\n",
        "        z=y.cpu().numpy()\n",
        "        #print(type(z))\n",
        "        #print('z shape === ',z.shape)\n",
        "        w=z.reshape(x.size(2),)\n",
        "        #print('w shape= ',w.shape)\n",
        "\n",
        "        #MB: 01.04.02 %2 %%%%%%%%%%%%%%%%%%%%%-------------------->>>>>>>\n",
        "        mfccs = librosa.feature.mfcc(w, sr=self.sampling_rate,n_mfcc=36, n_fft=1024, hop_length=258, win_length=1024)\n",
        "\n",
        "        #print('mfccs shape== ',mfccs.shape)\n",
        "        frame_number=mfccs.shape[1]\n",
        "        #print(self.n_mel_channels,frame_number)\n",
        "        #print('x.size(0)= ',type(x.size(0)))\n",
        "        t = torch.randn(x.size(0),self.n_mel_channels,frame_number)\n",
        "        #print('t.size()== ',t.size())\n",
        "        #print(x)\n",
        "        ####\n",
        "\n",
        "        ####\n",
        "        for i in np.arange(x.size(0)):\n",
        "            #print(i)\n",
        "            y=x[i,:,:]\n",
        "            #print('y= ',y.size())\n",
        "            z=y.cpu().numpy()\n",
        "            #print(type(z))\n",
        "            #print('z shape === ',z.shape)\n",
        "            w=z.reshape(x.size(2),)\n",
        "            #print('w shape=== ',w.shape)\n",
        "            #MB: 01.04.02 %3 %%%%n_mfcc =24%%%%%%%%%%%%%%%%%------------>>>>>>\n",
        "            mfccs = librosa.feature.mfcc(w, sr=self.sampling_rate,n_mfcc=36, n_fft=1024, hop_length=258, win_length=1024)\n",
        "\n",
        "            Q=torch.tensor(mfccs)\n",
        "            #print('Q== ',Q.size())\n",
        "            #print(t[i,:,:])\n",
        "            t[i,:,:]=Q\n",
        "\n",
        "            #print(t[i,:,:])\n",
        "        return t\n",
        "######"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9OUim_fIuEP"
      },
      "source": [
        "# MRH: Original MelGAN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-06T08:14:47.461957Z",
          "iopub.status.busy": "2023-07-06T08:14:47.461594Z",
          "iopub.status.idle": "2023-07-06T08:14:47.492745Z",
          "shell.execute_reply": "2023-07-06T08:14:47.491596Z",
          "shell.execute_reply.started": "2023-07-06T08:14:47.461929Z"
        },
        "id": "fpxMQ-CtIuEP",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "########### Model ##############\n",
        "\n",
        "from torch.nn.utils import weight_norm\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find(\"Conv\") != -1:\n",
        "        m.weight.data.normal_(0.0, 0.02)\n",
        "    elif classname.find(\"BatchNorm2d\") != -1:\n",
        "        m.weight.data.normal_(1.0, 0.02)\n",
        "        m.bias.data.fill_(0)\n",
        "\n",
        "\n",
        "def WNConv1d(*args, **kwargs):\n",
        "    return weight_norm(nn.Conv1d(*args, **kwargs))\n",
        "\n",
        "\n",
        "def WNConvTranspose1d(*args, **kwargs):\n",
        "    return weight_norm(nn.ConvTranspose1d(*args, **kwargs))\n",
        "################\n",
        "class ResnetBlock(nn.Module):\n",
        "    def __init__(self, dim, dilation=1):\n",
        "        super().__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.ReflectionPad1d(dilation),\n",
        "            WNConv1d(dim, dim, kernel_size=3, dilation=dilation),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            WNConv1d(dim, dim, kernel_size=1),\n",
        "        )\n",
        "        self.shortcut = WNConv1d(dim, dim, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.shortcut(x) + self.block(x)\n",
        "#######\n",
        "####################################################################################################\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, input_size, ngf, n_residual_layers):\n",
        "        super().__init__()\n",
        "        ratios = [8, 8, 2, 2]\n",
        "        self.hop_length = np.prod(ratios)\n",
        "        mult = int(2 ** len(ratios))\n",
        "\n",
        "        model = [\n",
        "            nn.ReflectionPad1d(3),\n",
        "            WNConv1d(input_size, mult * ngf, kernel_size=7, padding=0),\n",
        "        ]\n",
        "        MRH_in=32\n",
        "        # Upsample to raw audio scale\n",
        "        for i, r in enumerate(ratios):\n",
        "            #print('mult= ',mult)\n",
        "            model += [\n",
        "                nn.LeakyReLU(0.2),\n",
        "                WNConvTranspose1d(\n",
        "                    mult * ngf,\n",
        "                    mult * ngf // 2,\n",
        "                    kernel_size=r * 2,\n",
        "                    stride=r,\n",
        "                    padding=r // 2 + r % 2,\n",
        "                    output_padding=r % 2,\n",
        "                ),\n",
        "            ]\n",
        "            #MRH\n",
        "            #print('output channel= ',mult * ngf // 2)\n",
        "            kernel_size=r * 2\n",
        "            stride=r\n",
        "            padding=r // 2 + r % 2\n",
        "            output_padding=r % 2\n",
        "            #MRH_dilation=1\n",
        "            #MRH_length=(MRH_in-1)*stride-2*padding +MRH_dilation*(kernel_size-1)+ output_padding + 1\n",
        "            #MRH_in=MRH_length\n",
        "            #print(input_size)\n",
        "            #print(\"MRH calculated output Length= \",MRH_length)\n",
        "\n",
        "            for j in range(n_residual_layers):\n",
        "                model += [ResnetBlock(mult * ngf // 2, dilation=3 ** j)]\n",
        "\n",
        "            mult //= 2\n",
        "\n",
        "        model += [\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.ReflectionPad1d(3),\n",
        "            WNConv1d(ngf, 1, kernel_size=7, padding=0),\n",
        "            nn.Tanh(),\n",
        "        ]\n",
        "\n",
        "        self.model = nn.Sequential(*model)\n",
        "        self.apply(weights_init)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #print(x.size())\n",
        "        return self.model(x)\n",
        "#######\n",
        "############################################################################\n",
        "class MyModule(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyModule, self).__init__()\n",
        "        self.choices = nn.ModuleDict({\n",
        "                'conv': nn.Conv2d(10, 10, 3),\n",
        "                'pool': nn.MaxPool2d(3)\n",
        "        })\n",
        "        self.activations = nn.ModuleDict([\n",
        "                ['lrelu', nn.LeakyReLU()],\n",
        "                ['prelu', nn.PReLU()]\n",
        "        ])\n",
        "\n",
        "    def forward(self, x, choice='conv', act='prelu'):\n",
        "        x = self.choices[choice](x)\n",
        "        #print(x.size())\n",
        "        x = self.activations[act](x)\n",
        "        #print(x.size())\n",
        "        return x\n",
        "#####################################################################################################\n",
        "class NLayerDiscriminator(nn.Module):\n",
        "    def __init__(self, ndf, n_layers, downsampling_factor):\n",
        "        super().__init__()\n",
        "        model = nn.ModuleDict()\n",
        "\n",
        "        model[\"layer_0\"] = nn.Sequential(\n",
        "            nn.ReflectionPad1d(7),\n",
        "            WNConv1d(1, ndf, kernel_size=15),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "        )\n",
        "\n",
        "        nf = ndf\n",
        "        stride = downsampling_factor\n",
        "        for n in range(1, n_layers + 1):\n",
        "            nf_prev = nf\n",
        "            nf = min(nf * stride, 1024)\n",
        "\n",
        "            model[\"layer_%d\" % n] = nn.Sequential(\n",
        "                WNConv1d(\n",
        "                    nf_prev,\n",
        "                    nf,\n",
        "                    kernel_size=stride * 10 + 1,\n",
        "                    stride=stride,\n",
        "                    padding=stride * 5,\n",
        "                    groups=nf_prev // 4,\n",
        "                ),\n",
        "                nn.LeakyReLU(0.2, True),\n",
        "            )\n",
        "\n",
        "        nf = min(nf * 2, 1024)\n",
        "        model[\"layer_%d\" % (n_layers + 1)] = nn.Sequential(\n",
        "            WNConv1d(nf_prev, nf, kernel_size=5, stride=1, padding=2),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "        )\n",
        "\n",
        "        model[\"layer_%d\" % (n_layers + 2)] = WNConv1d(\n",
        "            nf, 1, kernel_size=3, stride=1, padding=1\n",
        "        )\n",
        "\n",
        "        self.model = model\n",
        "\n",
        "    def forward(self, x):\n",
        "        results = []\n",
        "        for key, layer in self.model.items():\n",
        "            x = layer(x)\n",
        "            results.append(x)\n",
        "        return results\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, num_D, ndf, n_layers, downsampling_factor):\n",
        "        super().__init__()\n",
        "        self.model = nn.ModuleDict()\n",
        "        for i in range(num_D):\n",
        "            self.model[f\"disc_{i}\"] = NLayerDiscriminator(\n",
        "                ndf, n_layers, downsampling_factor\n",
        "            )\n",
        "\n",
        "        self.downsample = nn.AvgPool1d(4, stride=2, padding=1, count_include_pad=False)\n",
        "        self.apply(weights_init)\n",
        "\n",
        "    def forward(self, x):\n",
        "        results = []\n",
        "        for key, disc in self.model.items():\n",
        "            #print('key= ', key)\n",
        "            #print('discrimiator= ',disc)\n",
        "            results.append(disc(x))\n",
        "            #print('Length results== ',len(results))\n",
        "            #print('results', results)\n",
        "            x = self.downsample(x)\n",
        "        return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MM49PGuIuEP"
      },
      "source": [
        "# MRH: HyperParameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-06T08:14:56.657959Z",
          "iopub.status.busy": "2023-07-06T08:14:56.657577Z",
          "iopub.status.idle": "2023-07-06T08:14:56.665095Z",
          "shell.execute_reply": "2023-07-06T08:14:56.664057Z",
          "shell.execute_reply.started": "2023-07-06T08:14:56.657927Z"
        },
        "id": "vY968J9mIuEP",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "n_mel_channels=30\n",
        "ngf=32\n",
        "n_residual_layers=3\n",
        "ndf=16\n",
        "num_D=3\n",
        "n_layers_D=4\n",
        "downsamp_factor=4\n",
        "lambda_feat=10\n",
        "#\n",
        "\n",
        "#batch_size = 16\n",
        "epochs     = 3000\n",
        "lr         = 1e-4\n",
        "save_interval = 500\n",
        "log_interval  = 200\n",
        "lambda_feat   = 1.0  # weight for feature‑matching loss\n",
        "batch_size=16\n",
        "seq_len=8192\n",
        "n_test_samples=10\n",
        "#####################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-06T12:07:24.952091Z",
          "iopub.status.busy": "2023-07-06T12:07:24.951001Z",
          "iopub.status.idle": "2023-07-06T12:07:24.956994Z",
          "shell.execute_reply": "2023-07-06T12:07:24.955929Z",
          "shell.execute_reply.started": "2023-07-06T12:07:24.952043Z"
        },
        "id": "foKnnTo-IuEP"
      },
      "source": [
        "# MRH:  Original Mel-GAN Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxZgt5cfIuEQ"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBtpOBbJIuEQ"
      },
      "source": [
        "## MRH:  In case of Using multiple GPUs:  using \"device\" instead of \"cuda\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\alber\\anaconda3\\envs\\CNN_Spectro\\lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
            "  WeightNorm.apply(module, name, dim)\n",
            "C:\\Users\\alber\\AppData\\Local\\Temp\\ipykernel_86652\\26343036.py:80: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()  # Initialize gradient scaler for mixed precision\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[16], line 169\u001b[0m\n\u001b[0;32m    166\u001b[0m                 start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 169\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[16], line 102\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     98\u001b[0m loss_D \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m d[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mmean() \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m D_fake) \\\n\u001b[0;32m     99\u001b[0m        \u001b[38;5;241m+\u001b[39m \u001b[38;5;28msum\u001b[39m(F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m r[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mmean() \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m D_real)\n\u001b[0;32m    101\u001b[0m optD\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 102\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_D\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m scaler\u001b[38;5;241m.\u001b[39mstep(optD)\n\u001b[0;32m    104\u001b[0m scaler\u001b[38;5;241m.\u001b[39mupdate()\n",
            "File \u001b[1;32mc:\\Users\\alber\\anaconda3\\envs\\CNN_Spectro\\lib\\site-packages\\torch\\_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    647\u001b[0m     )\n\u001b[1;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\alber\\anaconda3\\envs\\CNN_Spectro\\lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\alber\\anaconda3\\envs\\CNN_Spectro\\lib\\site-packages\\torch\\autograd\\graph.py:829\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    827\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    830\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    831\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    833\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "def main():\n",
        "    # 1) Output folder in the current directory\n",
        "    save_path = \"./MRH-Baseline\"\n",
        "    root = Path(save_path)\n",
        "    root.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    ####################################\n",
        "    # Create TensorBoard logger        #\n",
        "    ####################################\n",
        "    writer = SummaryWriter(str(root))\n",
        "\n",
        "    #######################\n",
        "    # Load PyTorch Models #\n",
        "    #######################\n",
        "    netG = Generator(n_mel_channels, ngf, n_residual_layers).cuda()\n",
        "    netD = Discriminator(num_D, ndf, n_layers_D, downsamp_factor).cuda()\n",
        "    fft  = mrh_Audio2Mel(n_mel_channels=n_mel_channels, sampling_rate=99840).cuda()\n",
        "\n",
        "    #####################\n",
        "    # Create optimizers #\n",
        "    #####################\n",
        "    optG = torch.optim.Adam(netG.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
        "    optD = torch.optim.Adam(netD.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
        "\n",
        "    #######################\n",
        "    # (Optional) resume?  #\n",
        "    #######################\n",
        "    # If loading from a checkpoint, uncomment and implement the loading mechanism here.\n",
        "\n",
        "    #######################\n",
        "    # Create data loaders #\n",
        "    #######################\n",
        "    train_files = files_to_list(\"train_files.txt\")\n",
        "    test_files  = files_to_list(\"test_files.txt\")\n",
        "    train_set = AudioDataset(train_files, segment_length=99840, sampling_rate=99840)\n",
        "    test_set  = AudioDataset(test_files, segment_length=99840, sampling_rate=99840, augment=False)\n",
        "\n",
        "    # Using multiple workers to load data in parallel\n",
        "    train_loader = DataLoader(train_set, batch_size=batch_size, num_workers=0, pin_memory=True)\n",
        "    test_loader  = DataLoader(test_set, batch_size=1, num_workers=0, pin_memory=True)\n",
        "\n",
        "    ##########################\n",
        "    # Dumping original audio #\n",
        "    ##########################\n",
        "    test_voc = []\n",
        "    test_audio = []\n",
        "    for i, x_t in enumerate(test_loader):\n",
        "        x_t = x_t.cuda()\n",
        "        s_t = fft(x_t).detach()\n",
        "\n",
        "        test_voc.append(s_t)\n",
        "        test_audio.append(x_t)\n",
        "\n",
        "        audio = x_t.squeeze().cpu()\n",
        "        save_sample(root / f\"original_{i}.wav\", 99840, audio)\n",
        "        writer.add_audio(f\"original/sample_{i}.wav\", audio, 0, sample_rate=99840)\n",
        "\n",
        "        if i == n_test_samples - 1:\n",
        "            break\n",
        "\n",
        "    costs = []\n",
        "    start = time.time()\n",
        "\n",
        "    # Enable cudnn autotuner\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    best_mel_reconst = 1e6\n",
        "    steps = 0\n",
        "\n",
        "    scaler = GradScaler()  # Initialize gradient scaler for mixed precision\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        for iterno, x_t in enumerate(train_loader):\n",
        "            x_t = x_t.cuda()\n",
        "            s_t = fft(x_t).detach()\n",
        "            x_pred_t = netG(s_t)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                s_pred_t = fft(x_pred_t)\n",
        "                s_error  = F.l1_loss(s_t, s_pred_t).item()\n",
        "\n",
        "            #######################\n",
        "            # Train Discriminator #\n",
        "            #######################\n",
        "            D_fake = netD(x_pred_t.detach())\n",
        "            D_real = netD(x_t)\n",
        "\n",
        "            loss_D = sum(F.relu(1 + d[-1]).mean() for d in D_fake) \\\n",
        "                   + sum(F.relu(1 - r[-1]).mean() for r in D_real)\n",
        "\n",
        "            optD.zero_grad()\n",
        "            scaler.scale(loss_D).backward()\n",
        "            scaler.step(optD)\n",
        "            scaler.update()\n",
        "\n",
        "            ###################\n",
        "            # Train Generator #\n",
        "            ###################\n",
        "            D_fake = netD(x_pred_t)\n",
        "\n",
        "            loss_G = sum(-d[-1].mean() for d in D_fake)\n",
        "\n",
        "            # feature matching loss computation (vectorized)\n",
        "            loss_feat = 0.0\n",
        "            feat_w = 4.0 / (n_layers_D + 1)\n",
        "            D_w    = 1.0 / num_D\n",
        "            w      = feat_w * D_w\n",
        "            for di in range(num_D):\n",
        "                loss_feat += w * sum(F.l1_loss(D_fake[di][lvl], D_real[di][lvl].detach()) for lvl in range(len(D_fake[di]) - 1))\n",
        "\n",
        "            optG.zero_grad()\n",
        "            scaler.scale((loss_G + lambda_feat * loss_feat)).backward()\n",
        "            scaler.step(optG)\n",
        "            scaler.update()\n",
        "\n",
        "            ######################\n",
        "            # Update tensorboard #\n",
        "            ######################\n",
        "            costs.append([loss_D.item(), loss_G.item(), loss_feat.item(), s_error])\n",
        "            writer.add_scalar(\"loss/discriminator\", costs[-1][0], steps)\n",
        "            writer.add_scalar(\"loss/generator\", costs[-1][1], steps)\n",
        "            writer.add_scalar(\"loss/feature_matching\", costs[-1][2], steps)\n",
        "            writer.add_scalar(\"loss/mel_reconstruction\", costs[-1][3], steps)\n",
        "            steps += 1\n",
        "\n",
        "            ###################\n",
        "            # Save & log pends#\n",
        "            ###################\n",
        "            if steps % save_interval == 0:\n",
        "                t0 = time.time()\n",
        "                with torch.no_grad():\n",
        "                    for i, voc in enumerate(test_voc):\n",
        "                        pred = netG(voc).squeeze().cpu()\n",
        "                        save_sample(root / f\"generated_{i}.wav\", 99840, pred)\n",
        "                        writer.add_audio(f\"generated/sample_{i}.wav\", pred, epoch, sample_rate=99840)\n",
        "\n",
        "                torch.save(netG.state_dict(), root / \"netG.pt\")\n",
        "                torch.save(optG.state_dict(), root / \"optG.pt\")\n",
        "                torch.save(netD.state_dict(), root / \"netD.pt\")\n",
        "                torch.save(optD.state_dict(), root / \"optD.pt\")\n",
        "\n",
        "                # Best-mel checkpoint\n",
        "                if np.mean(costs, axis=0)[-1] < best_mel_reconst:\n",
        "                    best_mel_reconst = np.mean(costs, axis=0)[-1]\n",
        "                    torch.save(netG.state_dict(), root / \"best_netG.pt\")\n",
        "                    torch.save(netD.state_dict(), root / \"best_netD.pt\")\n",
        "\n",
        "                print(f\"Took {time.time() - t0:.4f}s to generate samples\")\n",
        "                print(\"-\" * 100)\n",
        "\n",
        "            if steps % log_interval == 0:\n",
        "                avg = np.mean(costs, axis=0)\n",
        "                ms_per = 1000 * (time.time() - start) / log_interval\n",
        "                print(f\"Epoch {epoch} | Iters {iterno}/{len(train_loader)} | ms/batch {ms_per:.2f} | loss {avg}\")\n",
        "                costs = []\n",
        "                start = time.time()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-06T13:16:05.857628Z",
          "iopub.status.busy": "2023-07-06T13:16:05.857223Z",
          "iopub.status.idle": "2023-07-06T13:16:17.019127Z",
          "shell.execute_reply": "2023-07-06T13:16:17.017584Z",
          "shell.execute_reply.started": "2023-07-06T13:16:05.857586Z"
        },
        "id": "HP5lNW5dIuEQ",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\alber\\AppData\\Local\\Temp\\ipykernel_8980\\2535929146.py:5: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.\n",
            "  mel_basis = librosa.filters.mel(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19 | Iters 1/11 | ms/batch 17327.05 | loss [4.76733904 1.39048355 0.25639455 0.8285602 ]\n",
            "Epoch 37 | Iters 3/11 | ms/batch 16897.55 | loss [4.55331458 2.33303771 0.19102492 0.91983927]\n",
            "Took 4.2686s to generate samples\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 55 | Iters 5/11 | ms/batch 16039.86 | loss [4.37379098 2.72677717 0.1713358  0.86584703]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[57], line 184\u001b[0m\n\u001b[0;32m    180\u001b[0m                 start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 184\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[57], line 120\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    115\u001b[0m optD\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m###################\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;66;03m# Train Generator #\u001b[39;00m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;66;03m###################\u001b[39;00m\n\u001b[1;32m--> 120\u001b[0m D_fake \u001b[38;5;241m=\u001b[39m \u001b[43mnetD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_pred_t\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m loss_G \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;241m-\u001b[39md[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mmean() \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m D_fake)\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# feature matching\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\alber\\anaconda3\\envs\\CNN_Spectro\\lib\\site-packages\\torch\\nn\\modules\\module.py:1767\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1765\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1767\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\alber\\anaconda3\\envs\\CNN_Spectro\\lib\\site-packages\\torch\\nn\\modules\\module.py:1778\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1775\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1776\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1777\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1778\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1780\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "Cell \u001b[1;32mIn[55], line 182\u001b[0m, in \u001b[0;36mDiscriminator.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    178\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, disc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;66;03m#print('key= ', key)\u001b[39;00m\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;66;03m#print('discrimiator= ',disc)\u001b[39;00m\n\u001b[1;32m--> 182\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(\u001b[43mdisc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m#print('Length results== ',len(results))\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m#print('results', results)\u001b[39;00m\n\u001b[0;32m    185\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample(x)\n",
            "File \u001b[1;32mc:\\Users\\alber\\anaconda3\\envs\\CNN_Spectro\\lib\\site-packages\\torch\\nn\\modules\\module.py:1767\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1765\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1767\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\alber\\anaconda3\\envs\\CNN_Spectro\\lib\\site-packages\\torch\\nn\\modules\\module.py:1778\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1775\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1776\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1777\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1778\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1780\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "Cell \u001b[1;32mIn[55], line 160\u001b[0m, in \u001b[0;36mNLayerDiscriminator.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    158\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m--> 160\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(x)\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
            "File \u001b[1;32mc:\\Users\\alber\\anaconda3\\envs\\CNN_Spectro\\lib\\site-packages\\torch\\nn\\modules\\module.py:1767\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1765\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1767\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\alber\\anaconda3\\envs\\CNN_Spectro\\lib\\site-packages\\torch\\nn\\modules\\module.py:1778\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1775\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1776\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1777\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1778\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1780\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\alber\\anaconda3\\envs\\CNN_Spectro\\lib\\site-packages\\torch\\nn\\modules\\container.py:245\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 245\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\alber\\anaconda3\\envs\\CNN_Spectro\\lib\\site-packages\\torch\\nn\\modules\\module.py:1767\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1765\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1767\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\alber\\anaconda3\\envs\\CNN_Spectro\\lib\\site-packages\\torch\\nn\\modules\\module.py:1873\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1870\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[0;32m   1872\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1873\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1874\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1875\u001b[0m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[0;32m   1876\u001b[0m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[0;32m   1877\u001b[0m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks\u001b[38;5;241m.\u001b[39mitems():\n",
            "File \u001b[1;32mc:\\Users\\alber\\anaconda3\\envs\\CNN_Spectro\\lib\\site-packages\\torch\\nn\\modules\\module.py:1821\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1818\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m BackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[0;32m   1819\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[1;32m-> 1821\u001b[0m result \u001b[38;5;241m=\u001b[39m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[0;32m   1823\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[0;32m   1824\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[0;32m   1825\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[0;32m   1826\u001b[0m     ):\n\u001b[0;32m   1827\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\alber\\anaconda3\\envs\\CNN_Spectro\\lib\\site-packages\\torch\\nn\\modules\\conv.py:375\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\alber\\anaconda3\\envs\\CNN_Spectro\\lib\\site-packages\\torch\\nn\\modules\\conv.py:370\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(\n\u001b[0;32m    360\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    361\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    369\u001b[0m     )\n\u001b[1;32m--> 370\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "############  train.py ###########\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "import yaml\n",
        "import numpy as np\n",
        "import time\n",
        "import argparse\n",
        "from pathlib import Path\n",
        "\n",
        "###\n",
        "\n",
        "def main():\n",
        "    # 1) Output folder in the current directory\n",
        "    save_path = \"./MRH-Baseline\"\n",
        "    root = Path(save_path)\n",
        "    root.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    ####################################\n",
        "    # Create TensorBoard logger        #\n",
        "    ####################################\n",
        "    writer = SummaryWriter(str(root))\n",
        "\n",
        "    #######################\n",
        "    # Load PyTorch Models #\n",
        "    #######################\n",
        "    netG = Generator(n_mel_channels, ngf, n_residual_layers).cuda()\n",
        "    netD = Discriminator(num_D, ndf, n_layers_D, downsamp_factor).cuda()\n",
        "    # force 100 kHz here\n",
        "    fft  = mrh_Audio2Mel(n_mel_channels=n_mel_channels,\n",
        "                         sampling_rate=100000).cuda()\n",
        "\n",
        "    #####################\n",
        "    # Create optimizers #\n",
        "    #####################\n",
        "    optG = torch.optim.Adam(netG.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
        "    optD = torch.optim.Adam(netD.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
        "\n",
        "    #######################\n",
        "    # (Optional) resume?  #\n",
        "    #######################\n",
        "    # load_root = Path(load_path) if load_path else None\n",
        "    # if load_root and load_root.exists():\n",
        "    #     netG.load_state_dict(torch.load(load_root / \"netG.pt\"))\n",
        "    #     optG.load_state_dict(torch.load(load_root / \"optG.pt\"))\n",
        "    #     netD.load_state_dict(torch.load(load_root / \"netD.pt\"))\n",
        "    #     optD.load_state_dict(torch.load(load_root / \"optD.pt\"))\n",
        "\n",
        "    #######################\n",
        "    # Create data loaders #\n",
        "    #######################\n",
        "# ...existing code...\n",
        "    # now using 100 000 samples = 1 second\n",
        "    train_files = files_to_list(\"train_files.txt\")\n",
        "    test_files  = files_to_list(\"test_files.txt\")\n",
        "    train_set = AudioDataset(train_files, segment_length=100000, sampling_rate=100000)\n",
        "    test_set  = AudioDataset(test_files,  segment_length=100000, sampling_rate=100000, augment=False)\n",
        "# ...existing code...\n",
        "\n",
        "    train_loader = DataLoader(train_set, batch_size=batch_size, num_workers=0)\n",
        "    test_loader  = DataLoader(test_set,  batch_size=1, num_workers=0)\n",
        "\n",
        "    ##########################\n",
        "    # Dumping original audio #\n",
        "    ##########################\n",
        "    test_voc   = []\n",
        "    test_audio = []\n",
        "    for i, x_t in enumerate(test_loader):\n",
        "        x_t = x_t.cuda()\n",
        "        s_t = fft(x_t).detach()\n",
        "\n",
        "        test_voc.append(s_t)\n",
        "        test_audio.append(x_t)\n",
        "\n",
        "        audio = x_t.squeeze().cpu()\n",
        "        save_sample(root / f\"original_{i}.wav\", 100000, audio)\n",
        "        writer.add_audio(f\"original/sample_{i}.wav\", audio, 0, sample_rate=100000)\n",
        "\n",
        "        if i == n_test_samples - 1:\n",
        "            break\n",
        "\n",
        "    costs = []\n",
        "    start = time.time()\n",
        "\n",
        "    # enable cudnn autotuner\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    best_mel_reconst = 1e6\n",
        "    steps = 0\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        for iterno, x_t in enumerate(train_loader):\n",
        "            x_t = x_t.cuda()\n",
        "            s_t = fft(x_t).detach()\n",
        "            x_pred_t = netG(s_t)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                s_pred_t = fft(x_pred_t)\n",
        "                s_error  = F.l1_loss(s_t, s_pred_t).item()\n",
        "\n",
        "            #######################\n",
        "            # Train Discriminator #\n",
        "            #######################\n",
        "            D_fake = netD(x_pred_t.detach())\n",
        "            D_real = netD(x_t)\n",
        "\n",
        "            loss_D = sum(F.relu(1 + d[-1]).mean() for d in D_fake) \\\n",
        "                   + sum(F.relu(1 - r[-1]).mean() for r in D_real)\n",
        "\n",
        "            netD.zero_grad()\n",
        "            loss_D.backward()\n",
        "            optD.step()\n",
        "\n",
        "            ###################\n",
        "            # Train Generator #\n",
        "            ###################\n",
        "            D_fake = netD(x_pred_t)\n",
        "\n",
        "            loss_G = sum(-d[-1].mean() for d in D_fake)\n",
        "\n",
        "            # feature matching\n",
        "            loss_feat = 0.0\n",
        "            feat_w = 4.0 / (n_layers_D + 1)\n",
        "            D_w    = 1.0 / num_D\n",
        "            w      = feat_w * D_w\n",
        "            for di in range(num_D):\n",
        "                for lvl in range(len(D_fake[di]) - 1):\n",
        "                    loss_feat += w * F.l1_loss(\n",
        "                        D_fake[di][lvl],\n",
        "                        D_real[di][lvl].detach()\n",
        "                    )\n",
        "\n",
        "            netG.zero_grad()\n",
        "            (loss_G + lambda_feat * loss_feat).backward()\n",
        "            optG.step()\n",
        "\n",
        "            ######################\n",
        "            # Update tensorboard #\n",
        "            ######################\n",
        "            costs.append([loss_D.item(), loss_G.item(), loss_feat.item(), s_error])\n",
        "            writer.add_scalar(\"loss/discriminator\", costs[-1][0], steps)\n",
        "            writer.add_scalar(\"loss/generator\",     costs[-1][1], steps)\n",
        "            writer.add_scalar(\"loss/feature_matching\", costs[-1][2], steps)\n",
        "            writer.add_scalar(\"loss/mel_reconstruction\", costs[-1][3], steps)\n",
        "            steps += 1\n",
        "\n",
        "            ###################\n",
        "            # Save & log pends#\n",
        "            ###################\n",
        "            if steps % save_interval == 0:\n",
        "                t0 = time.time()\n",
        "                with torch.no_grad():\n",
        "                    for i, voc in enumerate(test_voc):\n",
        "                        pred = netG(voc).squeeze().cpu()\n",
        "                        save_sample(root / f\"generated_{i}.wav\", 100000, pred)\n",
        "                        writer.add_audio(f\"generated/sample_{i}.wav\", pred, epoch, sample_rate=100000)\n",
        "\n",
        "                torch.save(netG.state_dict(), root / \"netG.pt\")\n",
        "                torch.save(optG.state_dict(), root / \"optG.pt\")\n",
        "                torch.save(netD.state_dict(), root / \"netD.pt\")\n",
        "                torch.save(optD.state_dict(), root / \"optD.pt\")\n",
        "\n",
        "                # best‐mel checkpoint\n",
        "                if np.mean(costs, axis=0)[-1] < best_mel_reconst:\n",
        "                    best_mel_reconst = np.mean(costs, axis=0)[-1]\n",
        "                    torch.save(netG.state_dict(), root / \"best_netG.pt\")\n",
        "                    torch.save(netD.state_dict(), root / \"best_netD.pt\")\n",
        "\n",
        "                print(f\"Took {time.time()-t0:.4f}s to generate samples\")\n",
        "                print(\"-\" * 100)\n",
        "\n",
        "            if steps % log_interval == 0:\n",
        "                avg = np.mean(costs, axis=0)\n",
        "                ms_per = 1000 * (time.time() - start) / log_interval\n",
        "                print(f\"Epoch {epoch} | Iters {iterno}/{len(train_loader)} | ms/batch {ms_per:.2f} | loss {avg}\")\n",
        "                costs = []\n",
        "                start = time.time()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "CNN_Spectro",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
